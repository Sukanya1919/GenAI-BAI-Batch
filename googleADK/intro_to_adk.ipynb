{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fce81c",
   "metadata": {},
   "source": [
    "# Introduction to Google's ADK - Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-adk==1.5.0\n",
    "!pip install neo4j==5.28.1\n",
    "!pip install neo4j-graphrag\n",
    "!pip install  litellm==1.73.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j-graphrag\n",
      "  Downloading neo4j_graphrag-1.10.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting fsspec<2025.0.0,>=2024.9.0 (from neo4j-graphrag)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting json-repair<0.45.0,>=0.44.1 (from neo4j-graphrag)\n",
      "  Downloading json_repair-0.44.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from neo4j-graphrag) (5.28.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from neo4j-graphrag) (2.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from neo4j-graphrag) (2.11.7)\n",
      "Requirement already satisfied: pypdf<7.0.0,>=6.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from neo4j-graphrag) (6.0.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from neo4j-graphrag) (6.0.2)\n",
      "Collecting scipy<2.0.0,>=1.15.0 (from neo4j-graphrag)\n",
      "  Downloading scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting tenacity<10.0.0,>=9.1.2 (from neo4j-graphrag)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from neo4j-graphrag)\n",
      "  Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pytz in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from neo4j<6.0.0,>=5.17.0->neo4j-graphrag) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j-graphrag) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j-graphrag) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j-graphrag) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/genai/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.6.3->neo4j-graphrag) (0.4.0)\n",
      "Downloading neo4j_graphrag-1.10.0-py3-none-any.whl (201 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading json_repair-0.44.1-py3-none-any.whl (22 kB)\n",
      "Downloading scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-pyyaml, tenacity, scipy, json-repair, fsspec, neo4j-graphrag\n",
      "\u001b[2K  Attempting uninstall: tenacity\n",
      "\u001b[2K    Found existing installation: tenacity 8.5.0\n",
      "\u001b[2K    Uninstalling tenacity-8.5.0:\n",
      "\u001b[2K      Successfully uninstalled tenacity-8.5.0\n",
      "\u001b[2K  Attempting uninstall: json-repair\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: json_repair 0.25.2━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling json_repair-0.25.2:━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled json_repair-0.25.2━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: fsspecm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [neo4j-graphrag]m [neo4j-graphrag]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "crewai 0.175.0 requires json-repair==0.25.2, but you have json-repair 0.44.1 which is incompatible.\n",
      "google-adk 1.13.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0 json-repair-0.44.1 neo4j-graphrag-1.10.0 scipy-1.16.2 tenacity-9.1.2 types-pyyaml-6.0.12.20250915\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba70d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='yX3zaMLSEPS91e8P1qnziQY', created=1760787912, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Yes, I am ready. How can I help you?\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=13, prompt_tokens=4, total_tokens=17, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
      "\n",
      " Gemini is ready\n"
     ]
    }
   ],
   "source": [
    "# MODEL_GPT = \"openai/gpt-4o\"\n",
    "\n",
    "# llm = LiteLlm(model=MODEL_GPT)\n",
    "# llm_small = LiteLlm(model=\"openai/gpt-4o-mini\")\n",
    "# # Test LLM with a direct call\n",
    "# print(llm.llm_client.completion(model=llm.model, \n",
    "#                                 messages=[{\"role\": \"user\", \n",
    "#                                            \"content\": \"Are you ready?\"}], \n",
    "#                                 tools=[]),\n",
    "#                                 )\n",
    "\n",
    "\n",
    "# print(\"\\n OpenAI is ready\")\n",
    "\n",
    "\n",
    "\n",
    "MODEL_GPT = \"gemini/gemini-2.0-flash\"\n",
    "os.environ['GEMINI_API_KEY'] = \"AIzaSyCf5LqrAlx4fd2CgyYYjZT94OGrMAe7kIU\"\n",
    "\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT)\n",
    "llm_small = LiteLlm(model=\"gemini/gemini-2.0-flash\")\n",
    "print(llm.llm_client.completion(model=llm.model, \n",
    "                                messages=[{\"role\": \"user\", \n",
    "                                           \"content\": \"Are you ready?\"}], \n",
    "                                tools=[]),\n",
    "                                )\n",
    "\n",
    "print(\"\\n Gemini is ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1293d",
   "metadata": {},
   "source": [
    "## 3.2. Explore `neo4j_for_adk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d28cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j!\n"
     ]
    }
   ],
   "source": [
    "# from neo4j_for_adk import graphdb\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# --- Neo4j Connection Config ---\n",
    "uri = \"bolt://localhost:7687\"   # You can also try \"neo4j://localhost:7687\"\n",
    "user = \"neo4j\"                  # Default username\n",
    "password = \"your_password\" # Replace with your actual password\n",
    "\n",
    "# --- Initialize the driver ---\n",
    "graphdb = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# --- (Optional) Test the connection ---\n",
    "with graphdb.session() as session:\n",
    "    greeting = session.run(\"RETURN 'Connected to Neo4j!' AS message\").single()\n",
    "    print(greeting[\"message\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab455a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_for_adk import graphdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4a2ea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'query_result': [{'message1': 'Neo4j is Ready!'}]}\n"
     ]
    }
   ],
   "source": [
    "neo4j_is_ready = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message1\")\n",
    "\n",
    "print(neo4j_is_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "898e2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wish(name: Optional[str] = \"User\") -> str:\n",
    "    \"\"\"Function to wish the user.\"\"\"\n",
    "    return f\"Hello, {name}! How can I assist you today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53bf4af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Rajesh! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wish(\"Rajesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3e9b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic tool -- send a parameterized cypher query\n",
    "def say_hello(person_name: str) -> dict:\n",
    "    \"\"\"Formats a welcome message to a named person. \n",
    "\n",
    "    Args:\n",
    "        person_name (str): the name of the person saying hello\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the query.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'query_result' key with an array of result rows.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    return graphdb.send_query(\"RETURN 'Hello to you, ' + $person_name AS reply\",\n",
    "    {\n",
    "        \"person_name\": person_name\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f0f012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'query_result': [{'reply': 'Hello to you, GenAI Bangalore Batch'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"GenAI Bangalore Batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df19a280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'query_result': [{'reply': 'Hello to you, GenAI Bangalore Batch'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"GenAI Bangalore Batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f068021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'hello_agent_v1' created.\n"
     ]
    }
   ],
   "source": [
    "# Define the Cypher Agent\n",
    "hello_agent = Agent(\n",
    "    name=\"hello_agent_v1\",\n",
    "    model=llm_small, # defined earlier in a variable\n",
    "    description=\"Has friendly chats with a user.\",\n",
    "    instruction=\"\"\"You are a helpful assistant, chatting with a user. \n",
    "                Be polite and friendly, introducing yourself and asking who the user is. \n",
    "\n",
    "                If the user provides their name, use the 'say_hello' tool to get a custom greeting.\n",
    "                If the tool returns an error, inform the user politely. \n",
    "                If the tool is successful, present the reply.\n",
    "                \"\"\",\n",
    "    tools=[say_hello], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{hello_agent.name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae89d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'hello_agent_v1' created.\n"
     ]
    }
   ],
   "source": [
    "# Define the Cypher Agent\n",
    "hello_agent1 = Agent(\n",
    "    name=\"hello_agent_v2\",\n",
    "    model=llm_small, # defined earlier in a variable\n",
    "    description=\"Has friendly chats with a user.\",\n",
    "    instruction=\"\"\"You are a helpful assistant, chatting with a user. \n",
    "                Be polite and friendly, introducing yourself and asking who the user is. \n",
    "\n",
    "                If the user provides their name, use the 'say_hello' tool to get a custom greeting.\n",
    "                If the tool returns an error, inform the user politely. \n",
    "                If the tool is successful, present the reply.\n",
    "                \"\"\",\n",
    "    tools=[say_hello], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{hello_agent.name}' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514dcdde",
   "metadata": {},
   "source": [
    "# Run The Agent\n",
    "\n",
    "To run an agent, you'll need some additional components namely an execution environment and memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17309dfc",
   "metadata": {},
   "source": [
    "### Create the Runner and SessionService\n",
    "\n",
    "\n",
    "Let's assume we have a single user talking to the agent in a single session. Let's create this user, the session and the runner:\n",
    "* `SessionService`: Responsible for managing conversation history and state for different users and sessions. The `InMemorySessionService` is a simple implementation that stores everything in memory, suitable for testing and simple applications. It keeps track of the messages exchanged.  \n",
    "* `Runner`: The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the `SessionService`, and yields events representing the progress of the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7dca91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = hello_agent.name + \"_app\"\n",
    "user_id = hello_agent.name + \"_user_1\"\n",
    "session_id = hello_agent.name + \"_session_1\"\n",
    "\n",
    "\n",
    "# Initialize a session service and a session\n",
    "session_service = InMemorySessionService()\n",
    "await session_service.create_session(app_name = app_name,\n",
    "                                    user_id = user_id,\n",
    "                                    session_id = session_id)\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(\n",
    "    agent = hello_agent,\n",
    "    app_name = app_name,\n",
    "    session_service = session_service\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f59424",
   "metadata": {},
   "source": [
    "# Run the Agent\n",
    "\n",
    "\n",
    "Here's what's happening:\n",
    " \n",
    "1. Package the user query into the ADK `Content` format.\n",
    "2. Call`runner.run_async` (providing it with user/session context and the new message)\n",
    "4. Iterate through the **Events** yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).  \n",
    "5. Identify and print the **final response** event using `event.is_final_response()`.\n",
    "\n",
    "**Why `async`?** Interactions with LLMs and potentially tools (like external APIs) are I/O-bound operations. Using `asyncio` allows the program to handle these operations efficiently without blocking execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bb8c60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Event] Author: hello_agent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'person_name': 'batch of GenAI Bangalore'\n",
      "    },\n",
      "    id='call_6b0253cf70294231acd42fe802ba',\n",
      "    name='say_hello'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_6b0253cf70294231acd42fe802ba',\n",
      "    name='say_hello',\n",
      "    response={\n",
      "      'query_result': [\n",
      "        {\n",
      "          'reply': 'Hello to you, batch of GenAI Bangalore'\n",
      "        },\n",
      "      ],\n",
      "      'status': 'success'\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text=\"\"\"Hello batch of GenAI Bangalore, it's a pleasure to meet you!\n",
      "\"\"\"\n",
      ")] role='model'\n",
      "<<< Agent Response: Hello batch of GenAI Bangalore, it's a pleasure to meet you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Hello, I'm batch of GenAI Bangalore.\"\n",
    "\n",
    "content = types.Content(role='user', parts=[types.Part(text=user_message)])\n",
    "\n",
    "final_response_text = \"Agent did not produce a final response.\" # Default will be replaced if the agent produces a final response.\n",
    "\n",
    "verbose = True\n",
    "\n",
    "async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "    if verbose:\n",
    "        print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "    if event.is_final_response():\n",
    "        if event.content and event.content.parts:\n",
    "            final_response_text = event.content.parts[0].text ## # Assuming text response in the first part\n",
    "\n",
    "        elif event.actions and event.actions.escalate:\n",
    "            final_response_text = f\"Agent escalated: {event.error_message or 'No specific message provided.'}\"\n",
    "        break\n",
    "print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7ed70",
   "metadata": {},
   "source": [
    "# Create Helper Class: AgentCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_caller = AgentCaller(agent, runner, user_id, session_id)\n",
    "# agent_caller1 = AgentCaller(agent1, runner1, user_id1, session_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9bfdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCaller:\n",
    "    \"\"\"A simple wrapper class for interacting with an ADK agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: Agent, runner: Runner, \n",
    "                 user_id: str, session_id: str):\n",
    "        \"\"\"Initialize the AgentCaller with required components.\"\"\"\n",
    "        self.agent = agent\n",
    "        self.runner = runner\n",
    "        self.user_id = user_id\n",
    "        self.session_id = session_id\n",
    "\n",
    "\n",
    "    def get_session(self):\n",
    "        return self.runner.session_service.get_session(app_name=self.runner.app_name, user_id=self.user_id, session_id=self.session_id)\n",
    "\n",
    "    \n",
    "    async def call(self, user_message: str, verbose: bool = False):\n",
    "        \"\"\"Call the agent with a query and return the response.\"\"\"\n",
    "        print(f\"\\n>>> User Message: {user_message}\")\n",
    "\n",
    "        # Prepare the user's message in ADK format\n",
    "        content = types.Content(role='user', parts=[types.Part(text=user_message)])\n",
    "\n",
    "        final_response_text = \"Agent did not produce a final response.\" \n",
    "        \n",
    "        # Key Concept: run_async executes the agent logic and yields Events.\n",
    "        # We iterate through events to find the final answer.\n",
    "        async for event in self.runner.run_async(user_id=self.user_id, session_id=self.session_id, new_message=content):\n",
    "            # You can uncomment the line below to see *all* events during execution\n",
    "            if verbose:\n",
    "                print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "            # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "            if event.is_final_response():\n",
    "                if event.content and event.content.parts:\n",
    "                    # Assuming text response in the first part\n",
    "                    final_response_text = event.content.parts[0].text\n",
    "                elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "                    final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "                break # Stop processing events once the final response is found\n",
    "\n",
    "        print(f\"<<< Agent Response: {final_response_text}\")\n",
    "        return final_response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ab67caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_agent_caller(agent: Agent, initial_state: Optional[Dict[str, Any]] = {}) -> AgentCaller:\n",
    "    \"\"\"Create and return an AgentCaller instance for the given agent.\"\"\"\n",
    "    app_name = agent.name + \"_app\"\n",
    "    user_id = agent.name + \"_user\"\n",
    "    session_id = agent.name + \"_session_01\"\n",
    "    \n",
    "    # Initialize a session service and a session\n",
    "    session_service = InMemorySessionService()\n",
    "    await session_service.create_session(\n",
    "        app_name=app_name,\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        state=initial_state\n",
    "    )\n",
    "    \n",
    "    runner = Runner(\n",
    "        agent=agent,\n",
    "        app_name=app_name,\n",
    "        session_service=session_service\n",
    "    )\n",
    "    \n",
    "    return AgentCaller(agent, runner, user_id, session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68995d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_agent_caller = await make_agent_caller(hello_agent)\n",
    "hello_agent_caller1 = await make_agent_caller(hello_agent1)\n",
    "async def run_conversation():\n",
    "    await hello_agent_caller.call(\"Hello I'm ABK\", verbose=True)\n",
    "\n",
    "    await hello_agent_caller.call(\"I am excited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48820131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Message: Hello I'm ABK\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'person_name': 'ABK'\n",
      "    },\n",
      "    id='call_26827c3396ff4e37a1f4b92ddd4e',\n",
      "    name='say_hello'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_26827c3396ff4e37a1f4b92ddd4e',\n",
      "    name='say_hello',\n",
      "    response={\n",
      "      'query_result': [\n",
      "        {\n",
      "          'reply': 'Hello to you, ABK'\n",
      "        },\n",
      "      ],\n",
      "      'status': 'success'\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text=\"\"\"Hello there, ABK! It's a pleasure to meet you.\n",
      "\"\"\"\n",
      ")] role='model'\n",
      "<<< Agent Response: Hello there, ABK! It's a pleasure to meet you.\n",
      "\n",
      "\n",
      ">>> User Message: I am excited\n",
      "<<< Agent Response: Great! I am happy to hear that. Is there anything I can do to help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "185ab047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Message: Who are you?\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text=\"\"\"I am hello_agent_v1, a friendly chatbot here to have a chat with you.\n",
      "\"\"\"\n",
      ")] role='model'\n",
      "<<< Agent Response: I am hello_agent_v1, a friendly chatbot here to have a chat with you.\n",
      "\n",
      "\n",
      ">>> User Message: are you excited to attend genaI batch\n",
      "<<< Agent Response: As a large language model, I don't experience emotions like excitement. However, I am ready to assist the attendees of the genAI batch with information and support.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def run_conversation():\n",
    "    await hello_agent_caller.call(\"Who are you?\", verbose=True)\n",
    "\n",
    "    await hello_agent_caller.call(\"are you excited to attend genaI batch\")\n",
    "\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a68a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Message: what batch Name did i give?\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text=\"\"\"You mentioned \"genaI batch\". Is that correct?\n",
      "\"\"\"\n",
      ")] role='model'\n",
      "<<< Agent Response: You mentioned \"genaI batch\". Is that correct?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def run_conversation():\n",
    "    await hello_agent_caller.call(\"what batch Name did i give?\", verbose=True)\n",
    "\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "535ddc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Message: My Name is Anjul Tiwari?\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'person_name': 'Anjul Tiwari'\n",
      "    },\n",
      "    id='call_c1c08a5691714aa8b73ae70ba86a',\n",
      "    name='say_hello'\n",
      "  )\n",
      ")] role='model'\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: False, Content: parts=[Part(\n",
      "  function_response=FunctionResponse(\n",
      "    id='call_c1c08a5691714aa8b73ae70ba86a',\n",
      "    name='say_hello',\n",
      "    response={\n",
      "      'query_result': [\n",
      "        {\n",
      "          'reply': 'Hello to you, Anjul Tiwari'\n",
      "        },\n",
      "      ],\n",
      "      'status': 'success'\n",
      "    }\n",
      "  )\n",
      ")] role='user'\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text=\"\"\"Hello to you, Anjul Tiwari! It's a pleasure to chat with you. I will do my best to remember your name.\n",
      "\"\"\"\n",
      ")] role='model'\n",
      "<<< Agent Response: Hello to you, Anjul Tiwari! It's a pleasure to chat with you. I will do my best to remember your name.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def run_conversation():\n",
    "    await hello_agent_caller.call(\"My Name is Anjul Tiwari?\", verbose=True)\n",
    "\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20d57b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Message: what was the name did i tell you?\n",
      "  [Event] Author: hello_agent_v1, Type: Event, Final: True, Content: parts=[Part(\n",
      "  text=\"\"\"You told me your name is Anjul Tiwari.\n",
      "\"\"\"\n",
      ")] role='model'\n",
      "<<< Agent Response: You told me your name is Anjul Tiwari.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def run_conversation():\n",
    "    await hello_agent_caller.call(\"what was the name did i tell you?\", verbose=True)\n",
    "\n",
    "await run_conversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
